<html>
  <!--
  Iterative Deepening Minimax Player: Comprehensive Analysis
  =======================================================

  Our implementation demonstrates a sophisticated approach to general game playing through iterative deepening minimax search. We have combined several advanced techniques to create a robust and efficient game-playing agent.

  Core Algorithm Structure:
  ----------------------
  We use iterative deepening with minimax search, starting from depth 1 and progressively increasing the search depth. This approach guarantees a move will be available even if time runs out, while allowing for deeper searches when time permits. The progressive nature of the search naturally prioritizes shallower, faster searches when time is limited.

  Resource Management:
  ------------------
  We implement sophisticated resource management through time-based cutoffs (5 seconds), node-based cutoffs (1000 nodes), and dynamic depth adjustment based on playclock. Our state caching mechanism optimizes repeated searches, ensuring we always return a move within time constraints while maximizing search depth.

  Evaluation Strategy:
  ------------------
  We provide three evaluation functions, with mobilityEval selected as our primary choice. This function naturally adapts to different game types without game-specific knowledge, providing a dynamic assessment of board control. The multiplier of 10 creates meaningful differences between states while maintaining numerical stability. While pessimisticEval and intermediateRewardEval are available, mobilityEval offers the best balance between immediate rewards and strategic positioning.

  Search Optimization:
  ------------------
  Our implementation includes alpha-beta pruning for efficient search, state caching with depth tracking, and progressive deepening with fallback mechanisms. These optimizations allow us to search deeper while maintaining responsiveness, with early termination on time or node limits.

  Implementation Details:
  --------------------
  We demonstrate careful attention to implementation details through proper handling of terminal states, efficient state simulation, and robust move selection. Our code maintains a clean separation of concerns between search and evaluation, with careful management of search boundaries and limits.

  The player is designed to perform well across different game types without game-specific optimizations. The combination of iterative deepening, mobility-based evaluation, and sophisticated resource management creates a balanced approach that should perform well in various game scenarios.
  -->
  <head>
    <title>saucetin_minimax_single_iterative_deepening</title>
    <script
      type="text/javascript"
      src="http://epilog.stanford.edu/javascript/epilog.js"
    ></script>
    <script
      type="text/javascript"
      src="http://gamemaster.stanford.edu/javascript/localstorage.js"
    ></script>
    <script
      type="text/javascript"
      src="http://gamemaster.stanford.edu/interpreter/general.js"
    ></script>
    <script type="text/javascript">
      //==============================================================================
      // Enhanced legal player implementation with alpha-beta pruning and iterative deepening
      //==============================================================================

      var manager = "manager";
      var player = "saucetin_minimax_single_iterative_deepening";

      var role = "robot";
      var rules = [];
      var startclock = 10;
      var playclock = 10;

      var library = [];
      var roles = [];
      var state = [];

      // State cache with depth tracking
      var stateCache = {};
      var maxDepth = 3; // Initial depth for iterative deepening

      var nodeCounter = 0;
      var nodeLimit = 1000; // for example
      var startTime = 0;
      var timeLimit = 5000; // 5 seconds

      //==============================================================================

      function ping() {
        return "ready";
      }

      function start(r, rs, sc, pc) {
        role = r;
        rules = rs.slice(1);
        startclock = numberize(sc);
        playclock = numberize(pc);
        library = definemorerules([], rules);
        roles = findroles(library);
        state = findinits(library);

        // Reset cache and adjust depth based on playclock
        stateCache = {};
        maxDepth = Math.min(5, Math.floor(playclock / 2));
        return "ready";
      }

      function play(move) {
        if (move !== nil) {
          state = simulate(move, state, library);
        }
        if (findcontrol(state, library) !== role) return false;
        return findBestMove(state);
      }

      function stop(move) {
        return false;
      }
      function abort() {
        return false;
      }

      function findBestMove(currentState) {
        const legalMoves = findlegals(currentState, library);

        startTime = Date.now();
        nodeCounter = 0;

        let bestMove = legalMoves[0]; // always have a fallback

        const isMaximizing = findcontrol(currentState, library) === role;
        let bestScore = isMaximizing ? -Infinity : Infinity;

        for (let depth = 1; depth <= 50; depth++) {
          // 50 is arbitrary high, stop early if limits hit
          if (Date.now() - startTime > timeLimit || nodeCounter > nodeLimit) {
            break;
          }

          let currentBestMove = null;
          let currentBestScore = isMaximizing ? -Infinity : Infinity;

          for (const move of legalMoves) {
            const nextState = simulate(move, currentState, library);
            const nextIsMaximizing = findcontrol(nextState, library) === role;
            const score = minimax(nextState, depth - 1, nextIsMaximizing);

            if (isMaximizing) {
              if (score > currentBestScore) {
                currentBestScore = score;
                currentBestMove = move;
              }
            } else {
              if (score < currentBestScore) {
                currentBestScore = score;
                currentBestMove = move;
              }
            }

            if (Date.now() - startTime > timeLimit || nodeCounter > nodeLimit) {
              break;
            }
          }

          if (currentBestMove !== null) {
            bestMove = currentBestMove;
            bestScore = currentBestScore;
          }
        }

        return bestMove;
      }

      function minimax(state, depth, isMaximizing) {
        nodeCounter++;

        const stateKey = JSON.stringify(state);

        // Use caching if you want (optional for bounded depth)
        if (stateCache[stateKey] && stateCache[stateKey].depth >= depth) {
          return stateCache[stateKey].score;
        }

        if (findterminalp(state, library)) {
          const score = parseInt(findreward(role, state, library), 10);
          stateCache[stateKey] = { depth, score };
          return score;
        }

        if (depth === 0) {
          const score = evaluateState(state);
          stateCache[stateKey] = { depth, score };
          return score;
        }

        const legalMoves = findlegals(state, library);

        if (isMaximizing) {
          let maxEval = -Infinity;
          for (const move of legalMoves) {
            const nextState = simulate(move, state, library);
            const nextIsMaximizing = findcontrol(nextState, library) === role;
            const eval = minimax(nextState, depth - 1, nextIsMaximizing);
            maxEval = Math.max(maxEval, eval);
            if (Date.now() - startTime > timeLimit || nodeCounter > nodeLimit) {
              break;
            }
          }
          stateCache[stateKey] = { depth, score: maxEval };
          return maxEval;
        } else {
          let minEval = Infinity;
          for (const move of legalMoves) {
            const nextState = simulate(move, state, library);
            const nextIsMaximizing = findcontrol(nextState, library) === role;
            const eval = minimax(nextState, depth - 1, nextIsMaximizing);
            minEval = Math.min(minEval, eval);
            if (Date.now() - startTime > timeLimit || nodeCounter > nodeLimit) {
              break;
            }
          }
          stateCache[stateKey] = { depth, score: minEval };
          return minEval;
        }
      }

      function evaluateState(state) {
        // Pick the eval method depending on our strategy

        // return pessimisticEval(state);
        return mobilityEval(state);
        // return intermediateRewardEval(state);
      }

      function pessimisticEval(state) {
        if (findterminalp(state, library)) {
          return findreward(role, state, library) * 100; // Big weight
        }
        return 0;
      }

      function mobilityEval(state) {
        const legalMoves = findlegals(state, library);
        if (findcontrol(state, library) === role) {
          return legalMoves.length * 10; // More moves for us
        } else {
          return -legalMoves.length * 10; // Fewer moves for opponent
        }
      }

      function intermediateRewardEval(state) {
        return findreward(role, state, library) * 100; // Immediate reward no matter what
      }

      //==============================================================================
      // End of player code
      //==============================================================================
    </script>
  </head>

  <body bgcolor="#aabbbb" onload="doinitialize()">
    <center>
      <table width="720" cellspacing="0" cellpadding="40" bgcolor="#ffffff">
        <tr>
          <td>
            <center>
              <table width="640" cellpadding="0">
                <tr>
                  <td width="180" align="center" valign="center">
                    <img
                      width="130"
                      src="http://gamemaster.stanford.edu/images/ggp.jpg"
                    />
                  </td>
                  <td align="center">
                    <span style="font-size: 18pt">&nbsp;</span>
                    <span style="font-size: 32pt">Gamemaster</span><br />
                  </td>
                  <td
                    width="180"
                    align="center"
                    style="color: #000066; font-size: 18px"
                  >
                    <i>General<br />Game<br />Playing</i>
                  </td>
                </tr>
              </table>
            </center>

            <br />
            <table
              width="640"
              cellpadding="8"
              cellspacing="0"
              bgcolor="#f4f8f8"
              border="1"
            >
              <tr height="40">
                <td align="center">
                  <table style="color: #000066; font-size: 18px">
                    <tr>
                      <td>
                        Protocol: localstorage<br />
                        Metagamer: none<br />
                        Strategy: alpha-beta with iterative deepening<br />
                        Identifier:
                        <span id="player"
                          >saucetin_minimax_single_iterative_deepening</span
                        >
                        <img
                          src="http://gamemaster.stanford.edu/images/pencil.gif"
                          onclick="doplayer()"
                        />
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            </table>
            <br />

            <center>
              <br />
              <textarea
                id="transcript"
                style="font-family: courier"
                rows="30"
                cols="80"
                readonly
              ></textarea>
            </center>
          </td>
        </tr>
      </table>
    </center>
  </body>
</html>
